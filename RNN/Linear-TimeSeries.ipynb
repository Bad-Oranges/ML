{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]='TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15052 entries, 0 to 15051\n",
      "Data columns (total 52 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   week_1   15052 non-null  float64\n",
      " 1   week_2   15052 non-null  float64\n",
      " 2   week_3   15052 non-null  float64\n",
      " 3   week_4   15052 non-null  float64\n",
      " 4   week_5   15052 non-null  float64\n",
      " 5   week_6   15052 non-null  float64\n",
      " 6   week_7   15052 non-null  float64\n",
      " 7   week_8   15052 non-null  float64\n",
      " 8   week_9   15052 non-null  float64\n",
      " 9   week_10  15052 non-null  float64\n",
      " 10  week_11  15052 non-null  float64\n",
      " 11  week_12  15052 non-null  float64\n",
      " 12  week_13  15052 non-null  float64\n",
      " 13  week_14  15052 non-null  float64\n",
      " 14  week_15  15052 non-null  float64\n",
      " 15  week_16  15052 non-null  float64\n",
      " 16  week_17  15052 non-null  float64\n",
      " 17  week_18  15052 non-null  float64\n",
      " 18  week_19  15052 non-null  float64\n",
      " 19  week_20  15052 non-null  float64\n",
      " 20  week_21  15052 non-null  float64\n",
      " 21  week_22  15052 non-null  float64\n",
      " 22  week_23  15052 non-null  float64\n",
      " 23  week_24  15052 non-null  float64\n",
      " 24  week_25  15052 non-null  float64\n",
      " 25  week_26  15052 non-null  float64\n",
      " 26  week_27  15052 non-null  float64\n",
      " 27  week_28  15052 non-null  float64\n",
      " 28  week_29  15052 non-null  float64\n",
      " 29  week_30  15052 non-null  float64\n",
      " 30  week_31  15052 non-null  float64\n",
      " 31  week_32  15052 non-null  float64\n",
      " 32  week_33  15052 non-null  float64\n",
      " 33  week_34  15052 non-null  float64\n",
      " 34  week_35  15052 non-null  float64\n",
      " 35  week_36  15052 non-null  float64\n",
      " 36  week_37  15052 non-null  float64\n",
      " 37  week_38  15052 non-null  float64\n",
      " 38  week_39  15052 non-null  float64\n",
      " 39  week_40  15052 non-null  float64\n",
      " 40  week_41  15052 non-null  float64\n",
      " 41  week_42  15052 non-null  float64\n",
      " 42  week_43  15052 non-null  float64\n",
      " 43  week_44  15052 non-null  float64\n",
      " 44  week_45  15052 non-null  float64\n",
      " 45  week_46  15052 non-null  float64\n",
      " 46  week_47  15052 non-null  float64\n",
      " 47  week_48  15052 non-null  float64\n",
      " 48  week_49  15052 non-null  float64\n",
      " 49  week_50  15052 non-null  float64\n",
      " 50  week_51  15052 non-null  float64\n",
      " 51  week_52  15052 non-null  float64\n",
      "dtypes: float64(52)\n",
      "memory usage: 6.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "             week_1        week_2        week_3        week_4        week_5  \\\ncount  15052.000000  15052.000000  15052.000000  15052.000000  15052.000000   \nmean       2.373991      2.374079      2.386760      2.369365      2.390925   \nstd        1.083650      1.095881      1.083819      1.098046      1.084391   \nmin       -8.009502     -7.992483     -4.744953     -5.632819     -6.097533   \n25%        1.791478      1.802839      1.814614      1.778552      1.833261   \n50%        2.563223      2.562490      2.562064      2.562889      2.571490   \n75%        3.147875      3.154662      3.157488      3.157654      3.163058   \nmax        5.065280      5.086267      5.094143      4.744124      4.778071   \n\n             week_6        week_7        week_8        week_9       week_10  \\\ncount  15052.000000  15052.000000  15052.000000  15052.000000  15052.000000   \nmean       2.380990      2.390650      2.370621      2.381802      2.363768   \nstd        1.083049      1.079991      1.099412      1.083249      1.104624   \nmin       -6.567091     -8.009502     -8.481057     -5.367929     -5.229729   \n25%        1.811782      1.828376      1.800528      1.812449      1.790343   \n50%        2.564377      2.570222      2.552132      2.562669      2.545383   \n75%        3.157990      3.156671      3.155012      3.158665      3.156912   \nmax        4.898907      4.885306      5.086267      5.149720      4.788320   \n\n       ...       week_43       week_44       week_45       week_46  \\\ncount  ...  15052.000000  15052.000000  15052.000000  15052.000000   \nmean   ...      2.369496      2.384121      2.383063      2.363420   \nstd    ...      1.091731      1.086182      1.081982      1.108570   \nmin    ...     -8.481057     -9.529989     -8.481057     -4.880071   \n25%    ...      1.788151      1.819005      1.805862      1.791998   \n50%    ...      2.561131      2.568527      2.563032      2.567855   \n75%    ...      3.160052      3.157440      3.156312      3.140620   \nmax    ...      5.180371      5.149720      5.180371      5.149720   \n\n            week_47       week_48       week_49       week_50       week_51  \\\ncount  15052.000000  15052.000000  15052.000000  15052.000000  15052.000000   \nmean       2.384468      2.385531      2.371326      2.387824      2.395491   \nstd        1.085829      1.095854      1.113099      1.089533      1.115738   \nmin       -5.675433     -4.675652     -6.425318     -8.435697     -9.529989   \n25%        1.806817      1.819124      1.804890      1.807686      1.836240   \n50%        2.568770      2.562895      2.568934      2.568471      2.590219   \n75%        3.158223      3.177197      3.172867      3.169394      3.179056   \nmax        4.751501      4.768388      4.702625      4.881179      5.143515   \n\n            week_52  \ncount  15052.000000  \nmean       2.390995  \nstd        1.077972  \nmin       -6.287438  \n25%        1.814050  \n50%        2.573671  \n75%        3.158112  \nmax        5.180371  \n\n[8 rows x 52 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>week_1</th>\n      <th>week_2</th>\n      <th>week_3</th>\n      <th>week_4</th>\n      <th>week_5</th>\n      <th>week_6</th>\n      <th>week_7</th>\n      <th>week_8</th>\n      <th>week_9</th>\n      <th>week_10</th>\n      <th>...</th>\n      <th>week_43</th>\n      <th>week_44</th>\n      <th>week_45</th>\n      <th>week_46</th>\n      <th>week_47</th>\n      <th>week_48</th>\n      <th>week_49</th>\n      <th>week_50</th>\n      <th>week_51</th>\n      <th>week_52</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>...</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n      <td>15052.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.373991</td>\n      <td>2.374079</td>\n      <td>2.386760</td>\n      <td>2.369365</td>\n      <td>2.390925</td>\n      <td>2.380990</td>\n      <td>2.390650</td>\n      <td>2.370621</td>\n      <td>2.381802</td>\n      <td>2.363768</td>\n      <td>...</td>\n      <td>2.369496</td>\n      <td>2.384121</td>\n      <td>2.383063</td>\n      <td>2.363420</td>\n      <td>2.384468</td>\n      <td>2.385531</td>\n      <td>2.371326</td>\n      <td>2.387824</td>\n      <td>2.395491</td>\n      <td>2.390995</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.083650</td>\n      <td>1.095881</td>\n      <td>1.083819</td>\n      <td>1.098046</td>\n      <td>1.084391</td>\n      <td>1.083049</td>\n      <td>1.079991</td>\n      <td>1.099412</td>\n      <td>1.083249</td>\n      <td>1.104624</td>\n      <td>...</td>\n      <td>1.091731</td>\n      <td>1.086182</td>\n      <td>1.081982</td>\n      <td>1.108570</td>\n      <td>1.085829</td>\n      <td>1.095854</td>\n      <td>1.113099</td>\n      <td>1.089533</td>\n      <td>1.115738</td>\n      <td>1.077972</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-8.009502</td>\n      <td>-7.992483</td>\n      <td>-4.744953</td>\n      <td>-5.632819</td>\n      <td>-6.097533</td>\n      <td>-6.567091</td>\n      <td>-8.009502</td>\n      <td>-8.481057</td>\n      <td>-5.367929</td>\n      <td>-5.229729</td>\n      <td>...</td>\n      <td>-8.481057</td>\n      <td>-9.529989</td>\n      <td>-8.481057</td>\n      <td>-4.880071</td>\n      <td>-5.675433</td>\n      <td>-4.675652</td>\n      <td>-6.425318</td>\n      <td>-8.435697</td>\n      <td>-9.529989</td>\n      <td>-6.287438</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.791478</td>\n      <td>1.802839</td>\n      <td>1.814614</td>\n      <td>1.778552</td>\n      <td>1.833261</td>\n      <td>1.811782</td>\n      <td>1.828376</td>\n      <td>1.800528</td>\n      <td>1.812449</td>\n      <td>1.790343</td>\n      <td>...</td>\n      <td>1.788151</td>\n      <td>1.819005</td>\n      <td>1.805862</td>\n      <td>1.791998</td>\n      <td>1.806817</td>\n      <td>1.819124</td>\n      <td>1.804890</td>\n      <td>1.807686</td>\n      <td>1.836240</td>\n      <td>1.814050</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.563223</td>\n      <td>2.562490</td>\n      <td>2.562064</td>\n      <td>2.562889</td>\n      <td>2.571490</td>\n      <td>2.564377</td>\n      <td>2.570222</td>\n      <td>2.552132</td>\n      <td>2.562669</td>\n      <td>2.545383</td>\n      <td>...</td>\n      <td>2.561131</td>\n      <td>2.568527</td>\n      <td>2.563032</td>\n      <td>2.567855</td>\n      <td>2.568770</td>\n      <td>2.562895</td>\n      <td>2.568934</td>\n      <td>2.568471</td>\n      <td>2.590219</td>\n      <td>2.573671</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.147875</td>\n      <td>3.154662</td>\n      <td>3.157488</td>\n      <td>3.157654</td>\n      <td>3.163058</td>\n      <td>3.157990</td>\n      <td>3.156671</td>\n      <td>3.155012</td>\n      <td>3.158665</td>\n      <td>3.156912</td>\n      <td>...</td>\n      <td>3.160052</td>\n      <td>3.157440</td>\n      <td>3.156312</td>\n      <td>3.140620</td>\n      <td>3.158223</td>\n      <td>3.177197</td>\n      <td>3.172867</td>\n      <td>3.169394</td>\n      <td>3.179056</td>\n      <td>3.158112</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>5.065280</td>\n      <td>5.086267</td>\n      <td>5.094143</td>\n      <td>4.744124</td>\n      <td>4.778071</td>\n      <td>4.898907</td>\n      <td>4.885306</td>\n      <td>5.086267</td>\n      <td>5.149720</td>\n      <td>4.788320</td>\n      <td>...</td>\n      <td>5.180371</td>\n      <td>5.149720</td>\n      <td>5.180371</td>\n      <td>5.149720</td>\n      <td>4.751501</td>\n      <td>4.768388</td>\n      <td>4.702625</td>\n      <td>4.881179</td>\n      <td>5.143515</td>\n      <td>5.180371</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 52 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x\n",
    "sale = pd.read_csv('../data/Time series data/sale.csv', index_col=0)\n",
    "# y\n",
    "revenue = pd.read_csv('../data/Time series data/revenue.csv', index_col=0)\n",
    "\n",
    "sale.info()\n",
    "sale.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 0 to 9999\n",
      "Data columns (total 60 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   day_1   10000 non-null  float64\n",
      " 1   day_2   10000 non-null  float64\n",
      " 2   day_3   10000 non-null  float64\n",
      " 3   day_4   10000 non-null  float64\n",
      " 4   day_5   10000 non-null  float64\n",
      " 5   day_6   10000 non-null  float64\n",
      " 6   day_7   10000 non-null  float64\n",
      " 7   day_8   10000 non-null  float64\n",
      " 8   day_9   10000 non-null  float64\n",
      " 9   day_10  10000 non-null  float64\n",
      " 10  day_11  10000 non-null  float64\n",
      " 11  day_12  10000 non-null  float64\n",
      " 12  day_13  10000 non-null  float64\n",
      " 13  day_14  10000 non-null  float64\n",
      " 14  day_15  10000 non-null  float64\n",
      " 15  day_16  10000 non-null  float64\n",
      " 16  day_17  10000 non-null  float64\n",
      " 17  day_18  10000 non-null  float64\n",
      " 18  day_19  10000 non-null  float64\n",
      " 19  day_20  10000 non-null  float64\n",
      " 20  day_21  10000 non-null  float64\n",
      " 21  day_22  10000 non-null  float64\n",
      " 22  day_23  10000 non-null  float64\n",
      " 23  day_24  10000 non-null  float64\n",
      " 24  day_25  10000 non-null  float64\n",
      " 25  day_26  10000 non-null  float64\n",
      " 26  day_27  10000 non-null  float64\n",
      " 27  day_28  10000 non-null  float64\n",
      " 28  day_29  10000 non-null  float64\n",
      " 29  day_30  10000 non-null  float64\n",
      " 30  day_31  10000 non-null  float64\n",
      " 31  day_32  10000 non-null  float64\n",
      " 32  day_33  10000 non-null  float64\n",
      " 33  day_34  10000 non-null  float64\n",
      " 34  day_35  10000 non-null  float64\n",
      " 35  day_36  10000 non-null  float64\n",
      " 36  day_37  10000 non-null  float64\n",
      " 37  day_38  10000 non-null  float64\n",
      " 38  day_39  10000 non-null  float64\n",
      " 39  day_40  10000 non-null  float64\n",
      " 40  day_41  10000 non-null  float64\n",
      " 41  day_42  10000 non-null  float64\n",
      " 42  day_43  10000 non-null  float64\n",
      " 43  day_44  10000 non-null  float64\n",
      " 44  day_45  10000 non-null  float64\n",
      " 45  day_46  10000 non-null  float64\n",
      " 46  day_47  10000 non-null  float64\n",
      " 47  day_48  10000 non-null  float64\n",
      " 48  day_49  10000 non-null  float64\n",
      " 49  day_50  10000 non-null  float64\n",
      " 50  day_51  10000 non-null  float64\n",
      " 51  day_52  10000 non-null  float64\n",
      " 52  day_53  10000 non-null  float64\n",
      " 53  day_54  10000 non-null  float64\n",
      " 54  day_55  10000 non-null  float64\n",
      " 55  day_56  10000 non-null  float64\n",
      " 56  day_57  10000 non-null  float64\n",
      " 57  day_58  10000 non-null  float64\n",
      " 58  day_59  10000 non-null  float64\n",
      " 59  day_60  10000 non-null  float64\n",
      "dtypes: float64(60)\n",
      "memory usage: 4.7 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "              day_1         day_2         day_3         day_4         day_5  \\\ncount  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \nmean       2.002066      2.002719      2.003328      2.002939      2.004232   \nstd        0.499634      0.498294      0.497039      0.495331      0.494949   \nmin        1.070030      1.070089      1.070173      1.070518      1.069969   \n25%        1.707308      1.714404      1.720282      1.717037      1.719595   \n50%        2.004109      2.004752      2.008498      2.006018      2.011828   \n75%        2.292451      2.294516      2.289924      2.288005      2.286993   \nmax        2.928596      2.929652      2.929081      2.929368      2.929602   \n\n              day_6         day_7         day_8         day_9        day_10  \\\ncount  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \nmean       2.003964      2.004189      2.004339      2.004379      2.004549   \nstd        0.495048      0.495222      0.496170      0.496590      0.497196   \nmin        1.070909      1.070078      1.071269      1.070016      1.070351   \n25%        1.726962      1.725642      1.731331      1.723191      1.724931   \n50%        2.010492      2.004480      2.002067      2.002794      2.003254   \n75%        2.296187      2.303175      2.299779      2.302181      2.295706   \nmax        2.928788      2.929327      2.929351      2.929720      2.929158   \n\n       ...        day_51        day_52        day_53        day_54  \\\ncount  ...  10000.000000  10000.000000  10000.000000  10000.000000   \nmean   ...      2.000464      2.000073      1.999827      2.000198   \nstd    ...      0.501038      0.502797      0.504425      0.504625   \nmin    ...      1.070473      1.070517      1.069987      1.071675   \n25%    ...      1.703203      1.704192      1.705191      1.705129   \n50%    ...      1.997093      1.997385      1.993821      1.997269   \n75%    ...      2.304005      2.295833      2.299089      2.294304   \nmax    ...      2.929978      2.929537      2.929144      2.929972   \n\n             day_55        day_56        day_57        day_58        day_59  \\\ncount  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \nmean       2.000265      2.000353      2.000598      2.000218      2.000220   \nstd        0.505075      0.504703      0.504843      0.503771      0.502892   \nmin        1.071669      1.070437      1.070300      1.070461      1.070185   \n25%        1.705592      1.698782      1.698782      1.702400      1.703778   \n50%        1.998801      1.999444      1.995747      1.999064      2.001048   \n75%        2.305782      2.311864      2.315050      2.311191      2.309248   \nmax        2.929591      2.929580      2.929995      2.930002      2.928401   \n\n             day_60  \ncount  10000.000000  \nmean       2.001334  \nstd        0.501385  \nmin        1.070698  \n25%        1.706322  \n50%        2.001889  \n75%        2.294744  \nmax        2.928707  \n\n[8 rows x 60 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day_1</th>\n      <th>day_2</th>\n      <th>day_3</th>\n      <th>day_4</th>\n      <th>day_5</th>\n      <th>day_6</th>\n      <th>day_7</th>\n      <th>day_8</th>\n      <th>day_9</th>\n      <th>day_10</th>\n      <th>...</th>\n      <th>day_51</th>\n      <th>day_52</th>\n      <th>day_53</th>\n      <th>day_54</th>\n      <th>day_55</th>\n      <th>day_56</th>\n      <th>day_57</th>\n      <th>day_58</th>\n      <th>day_59</th>\n      <th>day_60</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>...</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n      <td>10000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.002066</td>\n      <td>2.002719</td>\n      <td>2.003328</td>\n      <td>2.002939</td>\n      <td>2.004232</td>\n      <td>2.003964</td>\n      <td>2.004189</td>\n      <td>2.004339</td>\n      <td>2.004379</td>\n      <td>2.004549</td>\n      <td>...</td>\n      <td>2.000464</td>\n      <td>2.000073</td>\n      <td>1.999827</td>\n      <td>2.000198</td>\n      <td>2.000265</td>\n      <td>2.000353</td>\n      <td>2.000598</td>\n      <td>2.000218</td>\n      <td>2.000220</td>\n      <td>2.001334</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.499634</td>\n      <td>0.498294</td>\n      <td>0.497039</td>\n      <td>0.495331</td>\n      <td>0.494949</td>\n      <td>0.495048</td>\n      <td>0.495222</td>\n      <td>0.496170</td>\n      <td>0.496590</td>\n      <td>0.497196</td>\n      <td>...</td>\n      <td>0.501038</td>\n      <td>0.502797</td>\n      <td>0.504425</td>\n      <td>0.504625</td>\n      <td>0.505075</td>\n      <td>0.504703</td>\n      <td>0.504843</td>\n      <td>0.503771</td>\n      <td>0.502892</td>\n      <td>0.501385</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.070030</td>\n      <td>1.070089</td>\n      <td>1.070173</td>\n      <td>1.070518</td>\n      <td>1.069969</td>\n      <td>1.070909</td>\n      <td>1.070078</td>\n      <td>1.071269</td>\n      <td>1.070016</td>\n      <td>1.070351</td>\n      <td>...</td>\n      <td>1.070473</td>\n      <td>1.070517</td>\n      <td>1.069987</td>\n      <td>1.071675</td>\n      <td>1.071669</td>\n      <td>1.070437</td>\n      <td>1.070300</td>\n      <td>1.070461</td>\n      <td>1.070185</td>\n      <td>1.070698</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.707308</td>\n      <td>1.714404</td>\n      <td>1.720282</td>\n      <td>1.717037</td>\n      <td>1.719595</td>\n      <td>1.726962</td>\n      <td>1.725642</td>\n      <td>1.731331</td>\n      <td>1.723191</td>\n      <td>1.724931</td>\n      <td>...</td>\n      <td>1.703203</td>\n      <td>1.704192</td>\n      <td>1.705191</td>\n      <td>1.705129</td>\n      <td>1.705592</td>\n      <td>1.698782</td>\n      <td>1.698782</td>\n      <td>1.702400</td>\n      <td>1.703778</td>\n      <td>1.706322</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.004109</td>\n      <td>2.004752</td>\n      <td>2.008498</td>\n      <td>2.006018</td>\n      <td>2.011828</td>\n      <td>2.010492</td>\n      <td>2.004480</td>\n      <td>2.002067</td>\n      <td>2.002794</td>\n      <td>2.003254</td>\n      <td>...</td>\n      <td>1.997093</td>\n      <td>1.997385</td>\n      <td>1.993821</td>\n      <td>1.997269</td>\n      <td>1.998801</td>\n      <td>1.999444</td>\n      <td>1.995747</td>\n      <td>1.999064</td>\n      <td>2.001048</td>\n      <td>2.001889</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.292451</td>\n      <td>2.294516</td>\n      <td>2.289924</td>\n      <td>2.288005</td>\n      <td>2.286993</td>\n      <td>2.296187</td>\n      <td>2.303175</td>\n      <td>2.299779</td>\n      <td>2.302181</td>\n      <td>2.295706</td>\n      <td>...</td>\n      <td>2.304005</td>\n      <td>2.295833</td>\n      <td>2.299089</td>\n      <td>2.294304</td>\n      <td>2.305782</td>\n      <td>2.311864</td>\n      <td>2.315050</td>\n      <td>2.311191</td>\n      <td>2.309248</td>\n      <td>2.294744</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.928596</td>\n      <td>2.929652</td>\n      <td>2.929081</td>\n      <td>2.929368</td>\n      <td>2.929602</td>\n      <td>2.928788</td>\n      <td>2.929327</td>\n      <td>2.929351</td>\n      <td>2.929720</td>\n      <td>2.929158</td>\n      <td>...</td>\n      <td>2.929978</td>\n      <td>2.929537</td>\n      <td>2.929144</td>\n      <td>2.929972</td>\n      <td>2.929591</td>\n      <td>2.929580</td>\n      <td>2.929995</td>\n      <td>2.930002</td>\n      <td>2.928401</td>\n      <td>2.928707</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 60 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revenue.info()\n",
    "revenue.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "# hyper parameters\n",
    "num_epochs = 15\n",
    "seq_length = 10\n",
    "batch_size = 10\n",
    "learning_rate = 1e-2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(sale.values, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(10, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return self.linear(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('layer1.0.weight',\n              tensor([[ 0.1074,  0.0183,  0.0619, -0.2976, -0.2795,  0.2392,  0.2525, -0.1447,\n                        0.0180,  0.0637],\n                      [-0.2341, -0.2916, -0.0302, -0.1533,  0.0135,  0.2066,  0.2774,  0.1831,\n                        0.1204,  0.1406],\n                      [-0.2400,  0.0313, -0.1560, -0.2376, -0.0343,  0.0464, -0.2907, -0.2364,\n                        0.2562, -0.2473],\n                      [-0.1860,  0.2350,  0.1782,  0.0986, -0.1648, -0.3138, -0.0750, -0.3068,\n                        0.0051, -0.1460],\n                      [ 0.3065, -0.1889, -0.1869,  0.1801,  0.0846, -0.2887, -0.1421,  0.1728,\n                       -0.0908,  0.1239],\n                      [-0.0646, -0.1437, -0.1948, -0.0289, -0.0979, -0.1029, -0.2401, -0.2388,\n                       -0.3107,  0.2748],\n                      [ 0.0416, -0.2306, -0.1734,  0.0579,  0.2080, -0.1910,  0.0887, -0.2463,\n                       -0.1656, -0.3044],\n                      [ 0.2501, -0.2318, -0.0220, -0.0309, -0.0160,  0.0545, -0.2394,  0.1552,\n                        0.1148,  0.1955],\n                      [ 0.0846, -0.1245,  0.3151, -0.2915, -0.0626, -0.0524, -0.0998, -0.1167,\n                       -0.1396,  0.2437],\n                      [-0.1049,  0.3094, -0.2764, -0.2722, -0.1167, -0.1261,  0.0189, -0.2190,\n                        0.1175,  0.1394]], device='cuda:0')),\n             ('layer1.0.bias',\n              tensor([-0.1423, -0.2481, -0.0945,  0.0713, -0.2514,  0.3134, -0.1000, -0.2105,\n                      -0.2108,  0.2658], device='cuda:0')),\n             ('layer2.0.weight',\n              tensor([[-8.2456e-02, -1.6306e-01,  3.1050e-01,  1.5013e-01, -8.0323e-02,\n                       -2.9112e-01, -2.5374e-01,  8.5960e-02, -9.4059e-02,  2.1200e-01],\n                      [-7.9860e-02, -1.3749e-01,  3.0811e-01,  3.1608e-01, -1.7685e-01,\n                        1.8365e-01,  1.7870e-01, -1.5453e-01, -1.2634e-01, -5.0220e-02],\n                      [-2.0022e-02, -6.9467e-02,  3.0165e-01, -1.6983e-01,  8.3570e-02,\n                       -1.4313e-01,  2.2584e-01,  2.9738e-03,  2.3286e-01, -1.8361e-01],\n                      [ 7.6510e-02, -2.8206e-01,  2.6440e-01, -1.0437e-01,  3.8993e-02,\n                        2.6327e-01, -2.5268e-02,  8.0160e-02,  2.1883e-01,  3.0396e-01],\n                      [ 1.8169e-01,  2.4693e-01, -5.1407e-02,  3.9688e-02,  1.1994e-01,\n                        1.2127e-01, -2.8639e-01, -1.1333e-01, -6.3793e-02, -7.4660e-02],\n                      [ 1.7939e-01, -7.4822e-02, -2.6066e-01,  2.2444e-04, -2.9992e-01,\n                       -2.4690e-01, -2.4558e-01,  3.1535e-01,  1.9219e-01,  1.5137e-01],\n                      [ 3.1071e-02, -2.8184e-01, -2.0986e-01,  3.0939e-01, -8.5074e-02,\n                        1.2186e-01, -1.3889e-01,  1.5480e-01, -1.4912e-02, -2.4378e-01],\n                      [ 1.8630e-01,  5.6515e-02,  1.3590e-01,  1.9585e-02, -2.6914e-01,\n                        1.5294e-01, -2.9532e-01, -1.8974e-01, -3.0650e-01, -1.4597e-01],\n                      [ 1.7648e-01,  1.6236e-01,  9.1649e-02, -7.2226e-02,  2.0570e-02,\n                       -8.6724e-02, -2.6900e-01,  6.1032e-02, -1.1488e-02, -1.2040e-01],\n                      [-2.4735e-02, -3.1282e-01,  2.8032e-02,  1.7419e-01, -1.9866e-01,\n                        2.5737e-01,  3.1284e-01,  1.0311e-01,  1.8711e-01, -1.3659e-01]],\n                     device='cuda:0')),\n             ('layer2.0.bias',\n              tensor([ 0.1988, -0.2612, -0.0267, -0.0491, -0.1521, -0.2048,  0.0107, -0.2906,\n                      -0.2074, -0.1314], device='cuda:0')),\n             ('linear.weight',\n              tensor([[ 0.2631,  0.1474, -0.1720, -0.0787,  0.0404, -0.2359, -0.2954, -0.2993,\n                       -0.1796,  0.1757],\n                      [-0.0988, -0.0083, -0.0689,  0.1504,  0.0859,  0.1879,  0.0295,  0.1371,\n                       -0.2770, -0.0570],\n                      [-0.0249,  0.2069, -0.0034,  0.0915, -0.2953, -0.2990, -0.0037,  0.0381,\n                       -0.0095, -0.2281],\n                      [-0.2097,  0.2835,  0.0631,  0.1580,  0.1119,  0.1465, -0.1035, -0.2141,\n                       -0.2368,  0.0047],\n                      [-0.2957,  0.3055, -0.1237, -0.1282,  0.2500,  0.1655, -0.2481,  0.2167,\n                       -0.1742,  0.0098],\n                      [-0.0980, -0.1054,  0.0987,  0.1560,  0.2351,  0.0688,  0.2472,  0.2496,\n                       -0.1519,  0.3122],\n                      [ 0.3101, -0.1501, -0.3074, -0.0611,  0.2004, -0.0306,  0.0465,  0.0121,\n                       -0.0053,  0.1147],\n                      [ 0.1129,  0.1711, -0.1498,  0.1256,  0.0872, -0.1903, -0.2136, -0.0834,\n                        0.0787, -0.1946],\n                      [-0.0710, -0.2709, -0.0893, -0.2845, -0.1653, -0.0697, -0.0669,  0.0291,\n                        0.2345, -0.0643],\n                      [-0.3139,  0.2313, -0.0432,  0.1822, -0.0930,  0.0600, -0.3047,  0.0926,\n                        0.0815, -0.3128]], device='cuda:0')),\n             ('linear.bias',\n              tensor([-0.0420, -0.0241, -0.2239,  0.2124,  0.1245, -0.0317,  0.0006,  0.3027,\n                      -0.0689, -0.1217], device='cuda:0'))])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Linear().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       atches/s]\r"
     ]
    }
   ],
   "source": [
    "linear_loss = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    loss_list = []\n",
    "    for i, data in enumerate(tqdm(test_loader, desc=f'Epoch {epoch}/{num_epochs}', unit='batches', colour='cyan', leave=False)):\n",
    "        random_index = random.randint(0, data.shape[-1] - seq_length - 1)\n",
    "        x_train = data[:, random_index:random_index + seq_length].to(device).float()\n",
    "        y_train = data[:, random_index + 1:random_index + seq_length + 1].to(device).float()\n",
    "\n",
    "        outputs = model(x_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "    linear_loss[epoch] = np.mean(loss_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x261025262b0>]"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(linear_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0025,  1.4425,  3.2191,  2.6897,  1.6684,  2.9446,  2.4261,  0.5689,\n",
      "          3.4429,  3.3586,  2.9953,  1.7905,  3.0199,  2.2264,  0.9716,  3.1949,\n",
      "          3.1606,  2.7676,  1.8018,  2.8436,  2.1114,  1.1388,  2.9849,  3.0005,\n",
      "          2.6299,  1.6793,  2.8206,  2.2867, -0.6088,  3.8445,  3.7936,  3.4705,\n",
      "          1.6391,  3.3828,  2.5043, -2.3274,  4.3554,  4.1440,  3.8342,  1.8149,\n",
      "          3.4698,  2.2551,  1.7445,  2.8047,  2.4761,  1.0915,  3.1307,  2.9910,\n",
      "          2.5652,  1.9913,  2.4533,  0.9506],\n",
      "        [ 1.0986,  2.9680,  2.7484,  2.3579,  2.0006,  2.1000,  0.1583,  3.3308,\n",
      "          3.4824,  3.2458,  0.8655,  3.6089,  3.1335,  2.5837,  2.4543,  1.7875,\n",
      "          2.3804,  1.3441,  2.3919,  2.0577,  1.0310,  2.8703,  2.9753,  2.7175,\n",
      "          1.1099,  3.2450,  2.9793,  2.4723,  2.2466,  1.9574,  1.7820,  1.5537,\n",
      "          1.5410,  0.6283,  2.6088,  3.1146,  3.0960,  1.2419,  3.3156,  2.8300,\n",
      "          2.0976,  2.6680,  1.1045,  2.9386,  2.7442,  2.3682,  1.9449,  2.2011,\n",
      "         -0.4185,  3.6060,  3.6918,  3.4342],\n",
      "        [ 0.1601,  3.7399,  3.4798,  3.1182,  1.9831,  2.9677,  1.8185,  2.2729,\n",
      "          0.5746,  3.0792,  3.1972,  3.0082,  0.6528,  3.5892,  3.2577,  2.8213,\n",
      "          2.1859,  2.5348, -0.7744,  3.8357,  3.7768,  3.5024,  1.5020,  3.4528,\n",
      "          2.6425,  0.8917,  3.4614,  3.2204,  2.7361,  2.1903,  2.4549, -2.0942,\n",
      "          4.1785,  4.0790,  3.8040,  1.5687,  3.5572,  2.5549, -2.1348,  4.3400,\n",
      "          4.1114,  3.7947,  1.8516,  3.4358,  2.2031,  1.8734,  2.6377,  2.1813,\n",
      "          0.0928,  3.5333,  3.5670,  3.2551],\n",
      "        [ 2.4755, -3.7474,  4.4984,  4.3291,  4.0678,  1.6317,  3.6358,  2.4524,\n",
      "          0.9861,  3.4149,  3.2212,  2.7137,  2.1896,  2.4420, -1.9470,  4.1431,\n",
      "          4.0530,  3.7766,  1.5581,  3.5504,  2.5688, -1.5366,  4.2184,  4.0021,\n",
      "          3.6719,  1.8683,  3.3705,  2.1830,  1.8641,  2.6018,  2.1454,  0.2569,\n",
      "          3.4430,  3.5004,  3.1942,  1.4112,  3.3538,  2.7430,  1.6727,  3.0121,\n",
      "          2.4753,  0.6748,  3.4202,  3.3057,  2.9228,  1.8724,  2.9098,  2.0241,\n",
      "          1.6907,  2.4398,  2.1526,  0.2552],\n",
      "        [ 0.8807,  2.4375,  2.7793,  2.8066,  1.0746,  3.2345,  2.9508,  2.4653,\n",
      "          2.2269,  1.9614,  1.7378,  1.6615,  1.0225,  2.3033,  2.6536,  2.6839,\n",
      "          1.1105,  3.1367,  2.9079,  2.4502,  2.1348,  2.0848,  1.1212,  2.6817,\n",
      "          2.7689,  2.5509,  0.9464,  3.2317,  3.0872,  2.6913,  1.9342,  2.6472,\n",
      "          1.5979,  2.3261,  1.1015,  2.5430,  2.5058,  2.3128,  1.1401,  2.9316,\n",
      "          2.8797,  2.5234,  1.7314,  2.6629,  2.0686,  0.6416,  3.2379,  3.3259,\n",
      "          3.0197,  1.3545,  3.2910,  2.7920],\n",
      "        [ 3.8356,  3.2877,  2.7983,  2.4231,  2.2151,  1.7659,  2.1011,  0.4918,\n",
      "          3.0590,  3.2599,  3.0882,  0.0508,  3.8320,  3.5096,  3.1431,  2.0540,\n",
      "          2.9340,  1.6010,  2.5800,  1.5302,  2.1139, -0.1400,  3.3672,  3.5603,\n",
      "          3.3593,  0.4338,  3.8004,  3.3370,  2.8919,  2.3027,  2.4845,  0.5108,\n",
      "          3.3259,  3.2965,  3.0139,  1.4803,  3.2089,  2.6543,  1.5209,  3.0372,\n",
      "          2.6415,  1.6782,  2.8425,  2.3114, -0.2822,  3.7437,  3.6971,  3.3693,\n",
      "          1.6463,  3.3281,  2.4950, -2.1653],\n",
      "        [ 3.3238,  3.0140,  2.4915,  2.3036,  1.8927,  2.0097,  0.2679,  3.2139,\n",
      "          3.4231,  3.2133,  0.4416,  3.7498,  3.3467,  2.9171,  2.2329,  2.5866,\n",
      "         -1.7594,  4.1209,  4.0034,  3.7308,  1.5884,  3.5168,  2.5390, -1.6014,\n",
      "          4.2255,  4.0167,  3.6881,  1.8562,  3.3844,  2.2026,  1.8201,  2.6662,\n",
      "          2.2697, -0.9356,  3.9180,  3.8721,  3.5606,  1.5977,  3.4439,  2.5478,\n",
      "         -0.9181,  4.0577,  3.8696,  3.5260,  1.8669,  3.2980,  2.1849,  1.7707,\n",
      "          2.6634,  2.3157,  0.3767,  3.4412],\n",
      "        [ 2.6633,  2.4535,  1.2541,  2.9588,  2.7930,  2.3171,  2.1112,  1.8808,\n",
      "          1.5945,  1.7807, -0.4513,  3.4535,  3.7187,  3.4896,  0.3549,  3.8716,\n",
      "          3.3606,  2.9142,  2.3392,  2.4678,  0.8535,  3.1314,  3.0789,  2.7761,\n",
      "          1.5817,  2.9901,  2.4839,  1.0738,  3.2019,  3.0459,  2.5967,  2.0530,\n",
      "          2.4296,  0.4374,  3.2831,  3.3081,  3.0633,  1.2337,  3.3620,  2.8866,\n",
      "          2.1635,  2.6532,  0.6387,  3.2639,  3.1743,  2.8944,  1.5682,  3.0758,\n",
      "          2.5241,  1.0944,  3.2271,  3.0422],\n",
      "        [ 3.0348,  2.7550,  1.5592,  2.9826,  2.5014,  1.2258,  3.1094,  2.9129,\n",
      "          2.3992,  2.2020,  1.9036,  1.7694,  1.4322,  1.7874,  1.0145,  2.1745,\n",
      "          2.4364,  2.5210,  1.1587,  2.9708,  2.8162,  2.4074,  1.9718,  2.2470,\n",
      "         -0.1650,  3.5148,  3.5989,  3.3476,  1.1293,  3.5450,  2.9605,  2.2206,\n",
      "          2.7248,  0.6854,  3.2739,  3.1434,  2.8448,  1.6820,  2.9648,  2.3384,\n",
      "         -1.5038,  4.1176,  4.0015,  3.6844,  1.7127,  3.4494,  2.4066,  0.9468,\n",
      "          3.3768,  3.2238,  2.7533,  2.0899],\n",
      "        [ 2.4127, -0.3650,  3.8500,  3.7320,  3.3811,  1.8014,  3.2557,  2.2844,\n",
      "          1.2831,  3.0899,  2.9602,  2.4366,  2.1629,  2.0687,  1.2279,  2.5743,\n",
      "          2.6163,  2.3580,  1.2549,  2.9020,  2.7844,  2.3450,  1.9964,  2.1280,\n",
      "         -0.4616,  3.6189,  3.7176,  3.4535,  1.1551,  3.5812,  2.9376,  2.1300,\n",
      "          2.8148,  1.4237,  2.7407,  2.2175,  0.6929,  3.2732,  3.2909,  2.9615,\n",
      "          1.5555,  3.1447,  2.5723,  1.1892,  3.2056,  2.9744,  2.4409,  2.2610,\n",
      "          1.8871,  1.9249,  0.7907,  2.8023]], dtype=torch.float64)\n",
      "tensor([[ 3.0025,  1.4425,  3.2191,  2.6897,  1.6684,  2.9446,  2.4261,  0.5689,\n",
      "          3.4429,  3.3586],\n",
      "        [ 1.0986,  2.9680,  2.7484,  2.3579,  2.0006,  2.1000,  0.1583,  3.3308,\n",
      "          3.4824,  3.2458],\n",
      "        [ 0.1601,  3.7399,  3.4798,  3.1182,  1.9831,  2.9677,  1.8185,  2.2729,\n",
      "          0.5746,  3.0792],\n",
      "        [ 2.4755, -3.7474,  4.4984,  4.3291,  4.0678,  1.6317,  3.6358,  2.4524,\n",
      "          0.9861,  3.4149],\n",
      "        [ 0.8807,  2.4375,  2.7793,  2.8066,  1.0746,  3.2345,  2.9508,  2.4653,\n",
      "          2.2269,  1.9614],\n",
      "        [ 3.8356,  3.2877,  2.7983,  2.4231,  2.2151,  1.7659,  2.1011,  0.4918,\n",
      "          3.0590,  3.2599],\n",
      "        [ 3.3238,  3.0140,  2.4915,  2.3036,  1.8927,  2.0097,  0.2679,  3.2139,\n",
      "          3.4231,  3.2133],\n",
      "        [ 2.6633,  2.4535,  1.2541,  2.9588,  2.7930,  2.3171,  2.1112,  1.8808,\n",
      "          1.5945,  1.7807],\n",
      "        [ 3.0348,  2.7550,  1.5592,  2.9826,  2.5014,  1.2258,  3.1094,  2.9129,\n",
      "          2.3992,  2.2020],\n",
      "        [ 2.4127, -0.3650,  3.8500,  3.7320,  3.3811,  1.8014,  3.2557,  2.2844,\n",
      "          1.2831,  3.0899]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 1.7912,  3.2855,  2.9700,  1.6435,  2.7851,  2.3862,  0.6350,  3.3684,\n          3.4225,  2.2731],\n        [ 3.1510,  2.5757,  2.8094,  2.2396,  2.2573,  0.3416,  3.5286,  3.5538,\n          3.2658,  1.5375],\n        [ 3.6618,  3.1932,  3.1032,  1.8557,  2.6540,  1.8214,  2.5723,  0.5479,\n          3.1285,  2.4189],\n        [-1.7418,  3.7333,  4.1836,  3.7658,  1.7126,  3.6580,  2.5853,  1.0990,\n          3.2470,  2.7655],\n        [ 2.8132,  2.8645,  3.1039,  0.9086,  3.4361,  2.8518,  2.7423,  2.1855,\n          2.1095,  2.4948],\n        [ 3.5860,  2.6547,  2.8375,  2.1549,  1.6209,  2.1135,  0.6605,  3.2495,\n          3.0298,  2.6657],\n        [ 3.2335,  2.2935,  2.8031,  2.1609,  2.0847,  0.3276,  3.5168,  3.5005,\n          3.2199,  1.5978],\n        [ 2.7345,  1.1143,  3.4668,  2.8193,  2.3248,  2.2560,  2.0688,  1.5870,\n          1.8944,  3.0746],\n        [ 2.9015,  1.7641,  3.3001,  2.5088,  1.0516,  3.0319,  3.3027,  2.5518,\n          2.2392,  3.0809],\n        [-0.3104,  3.8107,  4.1029,  3.4352,  1.8383,  3.1608,  2.5347,  1.3816,\n          3.0947,  2.7096]], device='cuda:0', grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = next(iter(test_loader))\n",
    "print(test_data)\n",
    "x = test_data[:, :seq_length].to(device).float()\n",
    "print(x)\n",
    "model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}